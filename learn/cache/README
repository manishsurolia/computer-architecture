Below is an hierarchy of memory access (By processor) based on the speed.

                     Processor
                     ---------

                         /\
                        /  \ <---------------- Registers (Level 1)
                       /    \
                      --------
                     /        \  <------------- Cache (Level 2)
                    /          \
                   --------------
                  /              \ <------------ Main Memory (Level 3)
                 /                \
                --------------------
               /                    \ <--------- Hard Disk (Level 4)
              /                      \
             --------------------------

Registers are very fast to access, but they can hold only few bytes of data,
second is 'cache', which is embedded in the CPU chip. Caches are generally
small, so we need RAM(Main memory) to store our programs while execution and
then it is Hard disk where the program exists forever.

Note : In most systems, the memory is a true hierarchy, meaning that data
cannot be present in level 'i' unless it is also present in level 'i+1'.

Here, first we first need to understand the "effective memory access time" from
the perspective of the processor.

A processor may either need an instruction to execute or a data to operate on,
and that could be present in main memory. Hence, it needs to get it to the
registers.

The processor first goes to the cache to find data on a particular address,
and does not find it there, it is called a "Cache Miss".

Miss : Element was not found at that memory level.
Hit : Element found at the memory level.

Hit Rate : no. of hits / no. of total memory requests

Miss Rate : Total rate - Hit Rate

          : ((hit + miss) / total memory requests) - Hit Rate

          : 1 - Hit Rate (Since, 'Hit+Miss' is equal to total memory requests)

          : 1 - Hit Rate

Note: The above maths of 'miss rate' is like we are getting the remaining
quantity.

Access time : Let's say that when processor tries to find data/instruction at a
particular memory address and it finds it in the cache itself, then cache takes
3ns(3 nanoseconds) to provide this data to the processor and if data for that
particular address is not found in the cache, processor goes down to the main
memory for data lookup, and main memory takes 30ns(30 nano seconds) to provide
the data for that address.

Effective access time : (hit rate * time taken by cache for each hit) +
                        ((1 - hit rate) * time taken by main memory for hit)

Note: here, we are assuming that, whatever is not found in cache, is definitely
found in the main memory.

Now, lets say that we have hit rate as : 80% (0.80).

Hence, the Effective access time would be,

    = (0.80 * 3ns) + ((1-0.80) * 30ns)
    = 2.4 ns + 6 ns
    = 8.4 ns.

Here, with 80% hit rate, the effective access time is 8.4 nano seconds.

Lets assume, if we can increase the hit rate to 90%, then how much will be the
'effective access time'?

   = (0.90 * 3ns) + ((1-0.90) * 30ns)
   = 2.7 ns + 3ns
   = 5.7ns.

This just shows that if we can improve on the hit rate in cache, then the
"Effective access time" will be improved.

Question:
--------

To improve the hit rate, what data we should keep in the cache, that improves
our hit rate, and as a result, "Effective access time" will improve.

Answer:
------

The idea of "Locality of reference" comes into picture to decide, what memory
data to keep in the cache, that improves the hit rate. Below are two
considerations based on the experience we have faced so far with programs,

    - Once a data is accessed, the same data element will be used again soon. It
      is called "Temporal Locality".
    - Neighbours will also be used soon. It is called "Spatial Locality".

Hence, based on the above 2 observations, we can keep the data accessed and
nearby data in the cache memory.

Note : Each data which is kept in the cache memory is actually associated with
some RAM address. Meaning, cache does not represent its own address space, and
rather it holds a copy of data from RAM for a particular RAM addresses. Hence,
when some data is accessed with an address, CPU needs to check whether data
related to this RAM address is present in the cache memory or not. We will talk
about it later, on how it is done actually.

Let's now take an example to understand a use case around cache.
Suppose in one of our C program, we have this below code to be executed.

    char a[64];
    for (int i=0; i<64; i++) {
        a[i] = i;
    }

When these lines of code are executed, when the very first time array 'a' is
accessed for index '0', the data related to the address (at index '0') will be
checked in the cache memory, and CPU will not find it. It is a 'cache miss'.

Now, hardware will bring this data and the nearby data in the cache. Assume that
all the 64 bytes are brought in the cache by hardware and now, starting from
index '1' up to index '63', we have all the data present in cache.

So, effectively '98.4375%' times, CPU was able to get the data from the cache
itself.

Hence, approximate hit rate = 0.98
and miss rate = 1 - 0.98 = 0.02

Now, by the same old formula (explained above).

Effective access time : (hit rate * time taken by cache for each hit) +
                        ((1 - hit rate) * time taken by main memory for hit)

Assuming that cache takes 3ns and main memory takes 30ns to provide data.

Effective access time : (0.98 * 3ns) + (0.02 * 30ns)
                      = 2.94 + 0.6
                      = 3.54ns.

Now, lets try to find how much time it would have taken for CPU, if there was
no cache and it had to access everything from main memory.

Here, hit rate (in cache) will become 0, hence, by formula,

                    = (0 * 3ns) + ((1 - 0) * 30ns)
                    = 30ns

It is 10 times greater, then accessing memory from cache.

Note : The above thing is applicable not only for arrays and rather for any
memory access done.

For example if I am using a variable 'a' from below code, all the nearby data
will come in the cache.

    int a, b, c;
    a = 20; // This brings 'b' and 'c' as well in the cache.


Question:
--------
How certain RAM(main memory) data is mapped in cache?

Answer:
------

Copying the data from RAM to the cache is done by cache hardware. In order to
make sure that it is done properly in hardware, we divide RAM memory in a fix
size of virtual blocks.

Each cache is divided in fix size lines. For example,

  Tag                      CACHE LINE
            ________ ________ ________ ________
[Block id1] |       |        |        |        | Line 0 (4 bytes)
            -------- -------- -------- --------
[Block id2] |       |        |        |        | Line 1 (4 bytes)
            -------- -------- -------- --------
[Block id3] |       |        |        |        | Line 2 (4 bytes)
            -------- -------- -------- --------
[Block id4] |       |        |        |        | Line 3 (4 bytes)
            -------- -------- -------- --------
[Block id5] |       |        |        |        | Line 4 (4 bytes)
            -------- -------- -------- --------
[Block id6] |       |        |        |        | Line 5 (4 bytes)
            -------- -------- -------- --------
            |                                  |
            |                                  |
           \|/                                \|/

Here, in the above example, each line has capacity to hold 4 bytes. Hence, our
RAM has to be divided in blocks of memory where each block is of size 4 bytes.

Let's assume that we have this 32 bit address in RAM : 0x12345678

Now, we should divide this address in such a way that first half of the address
represents 4 bytes of memory, called block.

We know that, to represent 4 bytes addresses in memory, we just need 2 bits of
the address as below:

                         /|\                               |      /|\     |
                          |                                |       |      |
                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 0110 (0x12345676)   |    DATA1     |
                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 0111 (0x12345677)   |    DATA2     |
                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 1000 (0x12345678)   |    DATA3     |
                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 1001 (0x12345679)   |    DATA4     |
                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 1010 (0x1234568A)   |    DATA5     |
                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 1011 (0x1234567B)   |    DATA6     |
                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 1100 (0x1234568C)   |    DATA7     |
                                                           ----------------
                          |                                |       |      |
                         \|/                               |      \|/     |

Above, left size we have mentioned the address of each byte in RAM, and right
size is represented as the actual RAM.

Now, since our cache has size of only 4 bytes, then which 4 bytes in RAM, do we
consider as a block and copy in the cache.

It's easy. As 4 bytes of address range can be represented with just 2 bits in
the address, we just need to take the first 30 bits as block address (Block id),
and remaining 2 bits can be used to get a specific byte from the block.

For example, here, take this address (0x1234567A),

    0001 0010 0011 0100 0101 0110 0111 10 10
   |<----------------------------------->|

Here, first 30 bits are taken as block address, and last all (2 bits), make them
0 as below:

    0001 0010 0011 0100 0101 0110 0111 10 00
   |<----------------------------------->|

This 30-bit address is '0x12345678'. Hence, '0x12345678' is the block ID and
the 4 bytes of data, which is related to this 30-bit block address, will be
copied in cache from RAM in one of cache line and that cache line address
(Block id) will be '0x12345678'.

                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 1000 (0x12345678)   |    DATA3     |
                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 1001 (0x12345679)   |    DATA4     |
                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 1010 (0x1234568A)   |    DATA5     |
                                                           ----------------
    0001 0010 0011 0100 0101 0110 0111 1011 (0x1234567B)   |    DATA6     |
                                                           ----------------

Here, note that the first 30 bits are same, only the last 2 bits are different.
Hardware will now copy the 4 bytes of data for this block id (0x12345678), as
below.

               00       01       10       11
            ________ ________ ________ ________
[0x12345678]| DATA3 | DATA4  | DATA5  | DATA6  | Line 0 (4 bytes)
            -------- -------- -------- --------
[Block id2] |       |        |        |        | Line 1 (4 bytes)
            -------- -------- -------- --------
[Block id3] |       |        |        |        | Line 2 (4 bytes)
            -------- -------- -------- --------
[Block id4] |       |        |        |        | Line 3 (4 bytes)
            -------- -------- -------- --------
[Block id5] |       |        |        |        | Line 4 (4 bytes)
            -------- -------- -------- --------
[Block id6] |       |        |        |        | Line 5 (4 bytes)
            -------- -------- -------- --------
            |                                  |
            |                                  |
           \|/                                \|/

The above (on the top of this cache) mentioned 2 bits are called "Word id".

In summary, execution of a program is like this,
    (1) When a program is executed and an address is accessed, Processor first
        divides the address in 2 parts, (1) Block id (first 30 bits in this
        example), and (2) word id (last 2 bits in this example) and pass both
        these items to the cache.

    (2) Cache first checks whether it has data belonging to this block id or
        not. If present, it is called a "cache hit" and the cache uses the
        provided 'word id' and fetch the corresponding data and provides the
        data to the processor. Otherwise, it is called a "cache miss". In this
        case, cache just queries to the RAM to gets all the bytes for this block
        id. Note that, checking for the block id in the whole cache is done in
        parallel by the cache hardware. Hence, its very fast.

Question:
---------

Suppose, Each line in a CPU cache can hold 8 bytes of data and processor wants
to fetch the data at address "0x1265014A", in a 32-bit, byte addressable RAM.
Show, how and what memory items will be copied from RAM to the cache.

Answer:
-------

Since, each line in the CPU cache can hold up to 8 bytes, only last 3 bits of an
address are needed to represent 8 addresses. Hence, our block id will be
32 - 3 = 29 bits.

Hence,
        Block id : 29 bits.
        Word id : 3 bits.

To get the block id, processor will just mask the last 3 bits with 0.
To get the word id, processor will just mask the first 29 bits with 0.

     0x1265014A : 0001 0010 0110 0000 0001 0100 1010

To represent last 3 bits in a block, lets take a cache mask as below:

        CACHE_MASK = 0x7

Here, all last 3 bits are 1.

Below, we can create 2 macros to mask to the address to fetch the 'block id' and
'word id'.

#define WORD_MASK  CACHE_MASK   <---- 0000 0000 0000 0000 0000 0000 0000 0111
#define BLOCK_MASK ~CACHE_MASK  <---- 1111 1111 1111 1111 1111 1111 1111 1000

Now, block address/id is: Address    &   BLOCK_MASK
                        : 0x1265014A &   1111 1111 1111 1111 1111 1111 1111 1000
                        : 0x12650148

So, '0x12650148' is the address from where cache hardware will take all 8 bytes
and copy in one of the cache line.

                         /|\                               |      /|\     |
                          |                                |       |      |
                                                           ----------------
    0001 0010 0110 0101 0000 0001 0100 1000 (0x12650148)   |    DATA1     |
                                                           ----------------
    0001 0010 0110 0101 0000 0001 0100 1001 (0x12650149)   |    DATA2     |
                                                           ----------------
    0001 0010 0110 0101 0000 0001 0100 1010 (0x1265014A)   |    DATA3     |
                                                           ----------------
    0001 0010 0110 0101 0000 0001 0100 1011 (0x1265014B)   |    DATA4     |
                                                           ----------------
    0001 0010 0110 0101 0000 0001 0100 1100 (0x1265014C)   |    DATA5     |
                                                           ----------------
    0001 0010 0110 0101 0000 0001 0100 1101 (0x1265014D)   |    DATA6     |
                                                           ----------------
    0001 0010 0110 0101 0000 0001 0100 1110 (0x1265014E)   |    DATA7     |
                                                           ----------------
    0001 0010 0110 0101 0000 0001 0100 1111 (0x1265014F)   |    DATA8     |
                                                           ----------------
                          |                                |       |      |
                         \|/                               |      \|/     |

                      Address                                     RAM

After cache hardware copies data from RAM to the cache, cache may look like
below. It is just an example, In reality this memory block can go to any line in
the cache depending on the replacement logic used.

               00       01       10       11
             _____ _____ _____ _____ _____ _____ _____ _____
[0x12650148]|DATA1|DATA2|DATA3|DATA4|DATA5|DATA6|DATA7|DATA8| Line 0 (8 bytes)
             ----- ----- ----- ----- ----- ----- ----- -----
[ Block id ]|     |     |     |     |     |     |     |     | Line 1 (8 bytes)
             ----- ----- ----- ----- ----- ----- ----- -----
[ Block id ]|     |     |     |     |     |     |     |     | Line 2 (8 bytes)
             ----- ----- ----- ----- ----- ----- ----- -----
[ Block id ]|     |     |     |     |     |     |     |     | Line 3 (8 bytes)
             ----- ----- ----- ----- ----- ----- ----- -----
[ Block id ]|     |     |     |     |     |     |     |     | Line 4 (8 bytes)
             ----- ----- ----- ----- ----- ----- ----- -----
[ Block id ]|     |     |     |     |     |     |     |     | Line 5 (8 bytes)
             ----- ----- ----- ----- ----- ----- ----- -----
[ Block id ]|     |     |     |     |     |     |     |     | Line 6 (8 bytes)
             ----- ----- ----- ----- ----- ----- ----- -----
            |                                  |
            |                                  |
           \|/                                \|/

Now, if any of the address in this range (0x12650148 - 0x1265014F), is accessed,
The data is already present in the cache and processor does not need to fetch it
from the RAM. This makes program execution faster.

Types of cache memories:
-----------------------

Cache memories are majorly divided in 3 categories.

(1) Fully Associative Cache
(2) Direct Mapped Cache
(3) Set Associative Cache

Fully Associative Cache
-----------------------

'Fully Associative Cache' is fairly simple to understand. It is same as what is
explained in the above sections. From a requested memory address, cache hardware
just need to divide it in 2 parts, 'block id' and 'word id', get 'block id'
related whole block from RAM and store it in any one of the cache line.

Once all lines of the cache is full, and processor request for an address, for
which block is not present in the cache, cache hardware needs to replace an old
block line with the new one coming from RAM.

There can be multiple ways of doing it,
    - FIFO   : Replace the block that has been in the cache longest.
    - LRU    : Replace the block that has been least recently used(not used for
               long time).
    - LFU    : Replace the block that has been used frequently used.
    - Random : Replace a random block.

So far, it has been observed that Random method of replacing the old block line
in cache works must faster and cheaper, because it does not need any help from
the hardware to maintain any data or add any time to keep track of how much time
a block has been in the cache to be able to decide, which one to replace.

Note : Lots of processor gives us option to choose between different cache line
replacement algorithms. We can decide which one is suitable for our application
and set that in the CPU.

Benefits :
    - Least chance of thrashing. Thrashing is like, replacing the old block and
      then getting it back again later and again doing the same. This may really
      affect the performance of memory access.

Drawbacks :
    - Fully associative cache is expensive. Checking all the blocks in one shot
      for a block id, needs a lot of hardware and that makes it very expensive.


Direct mapping cache
--------------------

In a 'Fully Associative cache', a block of memory from RAM, could be saved at
any line in cache.

In a 'Direct mapping cache', block of memory from RAM is saved at a particular
line of cache.

Finding out exact line of the cache (where data is going to be copied) is done
with the use of address itself. This approach is also very simple and straight
forward.

Let's see how we find out exact line number from a RAM address.

To understand the concept of direct mapping cache, lets draw a cache, which has
8 lines in it.

Each line in this cache is capable of holding 4 bytes of block from RAM.


                   00       01       10       11
                ________ ________ ________ ________
    [TAG]       |       |        |        |        | Line 0 (000)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 1 (001)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 2 (010)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 3 (011)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 4 (100)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 5 (101)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 6 (110)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 7 (111)
                -------- -------- -------- --------

Since a line here is capable of holding 4 bytes of block, the 'word id' bits
will be last 2 bits of the address.

When an address is accessed by processor, we need to choose exact line of this
cache with the same address.

Let's see how to choose exact line of the cache with the address.

Suppose, CPU wants to access this address, 0xA234B64A in a 32-bit byte
addressable RAM.

The bit pattern for this address is as below,

    1010 0010 0011 0100 1011 0110 0100 1010

Since a line can hold 4 bytes, we have last 2 bits in this address for that.
Hence, divide the address as below:

    1010 0010 0011 0100 1011 0110 0100 10  10
    |<---------------------------------->|<->|
                    Block id

Here the block id is : 1010 0010 0011 0100 1011 0110 0100 10 = 0xA234B648
Hence, 4 bytes starting from RAM address '0xA234B648' has to be copied in a line
in the cache. That is '0xA234B648 to 0xA234B64B' address.

Now, to choose the exact line to copy these 4 bytes from RAM, we just use the
last 3 bits in the block id, as below:


    1010 0010 0011 0100 1011 0110 010 0 10  10
    |<----------------------------------->|<->|
                    Block id         |<-->|
                                      Cache
                                      Line
                                    Selector
                                      Bits
                                (3 bits Line ID)

The remaining bits become TAG for this line.
Hence,
    TAG     = 1010 0010 0011 0100 1011 0110 010 (27 bits).
    Line ID = 010 (3 bits)
    Word ID = 10 (2 bits).

With the above divide of these three elements from a 32-bit address, our data
always go to an exact cache line, represented by 3-bits Line ID.

Also, when processor needs to find data related to an address, cache does not
need a dedicated hardware that compares all lines of cache memory for a
'block id', as it already knows that which line will have the block data. Just
get the 3-bit line id from the 32-bit address, and compare the TAG (27 bit) with
the TAG in that line, if TAG is present, then we have the block data in the
line. Now, pass the word id to the cache to get exact byte value. If the TAG
does not match, get the data block of RAM and copy it here in this line of
cache.

If there is another data already present there in the same line, replace that
data with the new block of data. That's it. Its quite fast because we don't need
to use any replacement logic here as we do in a 'Fully Associative cache'.

Benefits:
    - Its very cheap, as we don't need any extra hardware for parallel
      comparisons.
    - Its very fast, as cache already knows the line number, and does not need
      to find it by comparing all the lines.

Drawbacks:
    - There are so many addresses in the RAM, which has same Line ids (3-bits in
      this example). Hence the problem of thrashing occurs very quickly even
      though so many line may be empty. For example, check the address
      '0xA234B646' and '0xEA76F644'. They both have line ID bits as, '001' and
      when the second address is fetched here, it will simply replace the first
      one in the cache, even though we have so many lines empty in the cache.
      This is called thrashing problem in cache.

Set Associative Cache
---------------------

'Set Associative cache' address the problem of thrashing in the 'Direct mapping
cache'.

In the example given for 'Direct mapping cache', we had a cache with 8 lines,
and we were selecting one of the line with the use of 3 bits and we had an issue
of thrashing due to that because so many addresses have the same 3-bits.

Now, to reduce the problem of thrashing with 'Direct mapping cache', we use
only two bits to select any one of the 8 lines in the cache. This way, we have
2 options to choose from, for each 2-bit pattern.

We can understand it more with below diagram,


                   00       01       10       11
                ________ ________ ________ ________
    [TAG]       |       |        |        |        | Line 0 (00)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 1 (00)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 2 (01)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 3 (01)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 4 (10)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 5 (10)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 6 (11)
                -------- -------- -------- --------
    [TAG]       |       |        |        |        | Line 7 (11)
                -------- -------- -------- --------


Let's see how to choose exact line of the cache with the address with the same
old example.

Suppose, CPU wants to access this address, 0xA234B64A in a 32-bit byte
addressable RAM.

The bit pattern for this address is as below,

    1010 0010 0011 0100 1011 0110 0100 1010

Since a line can hold 4 bytes, we have last 2 bits in this address for that.
Hence, divide the address as below:

    1010 0010 0011 0100 1011 0110 0100 10  10
    |<---------------------------------->|<->|
                    Block id              word
                                           id

Here the block id is : 1010 0010 0011 0100 1011 0110 0100 10 = 0xA234B648
Hence, 4 bytes starting from RAM address '0xA234B648' has to be copied in a line
in the cache. That is '0xA234B648 to 0xA234B64B' address.

Now, to choose the exact line to copy these 4 bytes from RAM, we use only the
last 2 bits in the block id, as below:


    1010 0010 0011 0100 1011 0110 010 0  10  10
    |<------------------------------------>|<->|
                    Block id           |<->|
                                       Cache
                                       Line
                                     Selector
                                       Bits
                                 (2 bits Line ID)

The remaining bits become TAG for this line.
Hence,
    TAG     = 1010 0010 0011 0100 1011 0110 0100 (28 bits).
    Line ID = 10 (2 bits)
    Word ID = 10 (2 bits).

Now, since the Line ID is '10', cache has 2 lines to store this in the
cache. If it finds the first line empty, it just copies the whole block to the
line. Otherwise, it checks the second line for the same line id. If second line
is also not empty, it needs to replace one of them, and replacement algorithm
can be any one of the used for 'Fully associative cache'.


                                  00       01       10       11
                               ________ ________ ________ ________
    [TAG]                      |       |        |        |        | Line 0 (00)
                               -------- -------- -------- --------
    [TAG]                      |       |        |        |        | Line 1 (00)
                               -------- -------- -------- --------
    [TAG]                      |       |        |        |        | Line 2 (01)
                               -------- -------- -------- --------
    [TAG]                      |       |        |        |        | Line 3 (01)
                               -------- -------- -------- --------
[1010001000110100101101100100] |  2A   |   1B   |   91   |   F0   | Line 4 (10)
                               -------- -------- -------- --------
    [TAG]                      |       |        |        |        | Line 5 (10)
                               -------- -------- -------- --------
    [TAG]                      |       |        |        |        | Line 6 (11)
                               -------- -------- -------- --------
    [TAG]                      |       |        |        |        | Line 7 (11)
                               -------- -------- -------- --------

Its called as "Set Associative Cache' as each 2-bit pattern is combination of
2 lines, called a SET. The number of lines in a set will vary depending on the
number of lines present in the cache.

When the size of a set is big, it becomes a 'fully associative cache'. A 'Fully
associative cache' is like a very big 'set associative cache'.

As we can see, the number of lines in each set increases/decreases based on 2
factors,
(1) The number of bits we choose from a 'block id' for 'Line id'.
(2) The number of lines we have in the cache.

It is also called a 'k-way set associative cache', where k is the number of
lines we have in a set. In the above example, it is called a '2-way set
associative cache'.

Flags:
-----

Each line in a cache has flags associated with it.

    <-  Flags ->|
    ---------------------------------------------------------------------
    | T | V | L | D |      TAG                 |        BLOCK DATA      |
    ---------------------------------------------------------------------
      |   |   |   |
      |   |   |   |
      |   |   |   |
      |   |   |   |
      |   |   |   |
      |   |   |   |-> Dirty : Identifies a line of data that has been written
      |   |   |               to, but not updated to the main memory.
      |   |   |
      |   |   |-> Lock : Lock a line of the cache to prevent replacement.
      |   |
      |   |-> Valid : Whether the line contains valid data/instruction or not?
      |
      |--> Type : Data/Instruction. (Needed because instruction memory can not
                                     be written by programs).

Note about the above bits:

(1) Type : The way, we have our program execution setup in different OS, we can
not have data and instruction both in the same block. Hence, a block will be
either a DATA block, or an instruction block.

(2) Valid : When a system boots up, all the lines in cache memory are invalid.
Hence the flag is marked as invalid with this flag for each line in the cache
memory. During CPU using this cache, when this line has to be marked as
'Invalid', all block data should be copied back to the main memory first.

(3) Lock : There are some pieces of code/data, which we always want to keep in
the cache. Specially, from operating systems. Such blocks of memory are used
very frequently until the system is up and running.

(4) Dirty : When a block of memory is copied to the cache in a Line, and that
line has been updated by processor, it is marked as Dirty line. This marking is
very important, because when a new block has to come and replace this line, the
old data in this line has to be copied back to the main memory.

Cache write policies:
--------------------

Cache write policies are important as they can affect the performance of our
system based on which write policy we select. There are majorly 2 types of
cache write policies,
    (1) Write Through : Every write to cache, also writes through to the main
                        memory. Means, just when the cache is updated, we update
                        the main memory as well. This policy is important when
                        there are multiple cores, working on the same data. For
                        example, suppose we have five threads, running on five
                        different cores and operating the same memory location.
                        In such situations, This policy seems good, but it has a
                        problem that it makes the bus congested with the writes
                        for blocks, which may not be needed immediately. There
                        is a mechanism to handle such situations using, 'write
                        through bus watching'.

    (2) Write back : Update main memory only when a dirty line is to be
                     replaced. It reduces the bus traffic, but again, the issue
                     is there when multiple cores are looking for the same
                     memory address.

Cache Hierarchy:
---------------

Every CPU has a cache embedded in it. That is called L1 cache. There can be more
than one cache, which can be shared between cores.

                      ---------------------
                      |                   |
     ----------       | ---------------   |  ------------------
     |        |<--------| Instruction |<--|--|                |
     |        |<--------| Cache       |<--|--|                |
     |        |   One | | (Split)     |   |  |  Unified       |
     |        |   Way | --------------|   |  |   Cache        |
     |  CPU   |       |                   |  |  (Both         |
     |        |       | ---------------   |  |   Data         |
     |        |<------->| Data        |<--|->|   and          |
     |        |<------->| Cache       |<--|->|   Instructions)|
     |        |  Two  | | (Split)     |   |  |                |
     |        |  Ways | --------------|   |  |                |
     ----------       |                   |  |                |
                      ---------------------  ------------------
                            L1 Cache            L2 Cache

Unified Cache : A cache memory, which holds both instructions and data, is
called a unified cache.

Split Cache : A cache, which contains only either instructions or data.

Here, in L1, we have split cache, and L2 is a unified cache.

Problem with split cache: If we fix the size of a cache to be able to store
data in one cache and instructions in another, then one cache may always be
almost empty and another one has frequent replacements. A computer architect
needs to decide the caches carefully for a processor based on the needs or
the program which are supposed to be executed.

A unified cache can change the ratio of data/instruction sizes in the cache, but
with split cache we don't have that flexibility.

References:
----------

(1) Memory Systems, by Bruce Jacob, David Wang, Spencer Ng.
(2) https://www.youtube.com/watch?v=Bz49xnKBH_0&list=PLxfrSxK7P38X7XfG4X8Y9cdOURvC7ObMF&index=73
(3) https://www.youtube.com/watch?v=A0vR-ks3hsQ&list=PLxfrSxK7P38X7XfG4X8Y9cdOURvC7ObMF&index=74
(4) https://www.youtube.com/watch?v=zocwH0g-qQM&list=PLxfrSxK7P38X7XfG4X8Y9cdOURvC7ObMF&index=75
(5) https://www.youtube.com/watch?v=gr5M9CULUZw&list=PLxfrSxK7P38X7XfG4X8Y9cdOURvC7ObMF&index=76
(6) https://www.youtube.com/watch?v=Y7q2ECeFWE8&list=PLxfrSxK7P38X7XfG4X8Y9cdOURvC7ObMF&index=77
